# -*- coding: utf-8 -*-
"""Multi-Layer Perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NMOKb5jS5tvkRTozvoGTW3GScM_0LJCq
"""

def initialize_weights(layers):
    """Initialize weights and biases."""
    for i in range(len(layers) - 1):
      weights = np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2 / layers[i]) #HE initialiation: Prevents weights from being too large or too small, avoiding vanishing or exploding gradients during training
      biases = np.zeros((1, layers[i + 1]))
      return weights, biases

      return weights, biases

"""## Activation Function"""

def relu(z):
  return np.maximum(0,z)

def sigmoid(z):
  return 1 / (1 + np.exp(-z))

def tanh(z):
  return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))

def relu_derivative(z):
  return np.where(z > 0, 1, 0)

def relu(x):
  return np.maximum(0,x)

def relu_derivative(x):
  return (x > 0).astype(float)

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
  return sigmoid(x) * (1 - sigmoid(x))

"""### Forward Propagation (Forward Pass)"""

def forward_propagation(X, weights, biases, activation_functions):
  """Perform forward propagation."""
  activation_functions = activation_functions if isinstance(activation_functions, list) else [activation_functions] * len(weights)
  activations = [X]
  for i in range(len(weights)):
    z = np.dot(activations[i], weights[i]) + biases[i]
    a = activation_functions[i](z)
    activations.append(a)
    return activations



def forward_propagation(X, weights, biases, activation_functions):
  """Perform forward propagation."""
  activation_functions = activation_functions if isinstance(activation_functions, list) else [activation_functions] * len(weights)
  activations = [X]
  for i in range(len(weights) - 1): # This was unintendedly outside the funciton definition
      X = relu(np.dot(X, weights[i]) + biases[i])
      activations.append(X)
  output = sigmoid(np.dot(X, weights[-1]) + biases[-1])
  activations.append(output)
  return activations



"""### Backward Propagation"""

def backward_propagation(activations, y, weights, activation_functions):
  """Perform backward propagation."""
  m = y.shape[1]
  activation_functions = activation_functions if isinstance(activation_functions, list) else [activation_functions] * len(weights)
  deltas = [None] * len(weights)
  deltas[-1] = (activations[-1] - y) * activation_functions[-1](activations[-1])
  for i in range(len(weights) - 2, -1, -1):
    deltas[i] = np
    deltas[i] = np.dot(deltas[i + 1], weights[i + 1].T) * activation_functions[i](activations[i])
  return deltas

def backward(X, y, activations, weights):
    """Perform backward propagation."""
    m = X.shape[0]
    deltas = [activations[-1] - y]
    for i in range(len(weights) - 1, 0, -1):
        deltas.insert(0, np.dot(deltas[0], weights[i].T) * relu_derivative(activations[i]))
    weight_grads = [np.dot(activations[i].T, deltas[i]) / m for i in range(len(weights))]
    bias_grads = [np.sum(deltas[i], axis=0, keepdims=True) / m for i in range(len(weights))]
    return weight_grads, bias_grads

"""# Training"""

def train(X, y, layers, learning_rate=0.1, epochs=1000):
    """Train the MLP."""
    weights, biases = initialize_weights(layers)
    for epoch in range(epochs):
        activations = forward(X, weights, biases)
        weight_grads, bias_grads = backward(X, y, activations, weights)
        for i in range(len(weights)):
            weights[i] -= learning_rate * weight_grads[i]
            biases[i] -= learning_rate * bias_grads[i]
        if epoch % 100 == 0:
            loss = np.mean((activations[-1] - y) ** 2)
            print(f"Epoch {epoch}, Loss: {loss:.4f}")
    return weights, biases

"""## Prediction"""

def predict(X, weights, biases):
    """Make predictions."""
    activations = forward(X, weights, biases)
    return activations[-1]





import numpy as np

class MLP:
    def __init__(self, layers=[3, 4, 1], learning_rate=0.1):
        """
        layers: List of neurons in each layer [input_size, hidden_size, output_size]
        """
        self.layers = layers
        self.learning_rate = learning_rate

        # Initialize weights and biases
        self.weights = []
        self.biases = []

        for i in range(len(layers)-1):
            # He initialization for better training
            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2/layers[i])
            b = np.zeros((1, layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def relu(self, x):
        """ReLU activation function"""
        return np.maximum(0, x)

    def relu_derivative(self, x):
        """Derivative of ReLU"""
        return np.where(x > 0, 1, 0)

    def sigmoid(self, x):
        """Sigmoid activation function"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

    def sigmoid_derivative(self, x):
        """Derivative of sigmoid"""
        sx = self.sigmoid(x)
        return sx * (1 - sx)

    def forward(self, X):
        """
        Forward propagation
        Stores activations for use in backpropagation
        """
        self.activations = [X]
        self.z_values = []

        # Hidden layers with ReLU
        for i in range(len(self.weights)-1):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            activation = self.relu(z)
            self.activations.append(activation)

        # Output layer with sigmoid
        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]
        self.z_values.append(z)
        output = self.sigmoid(z)
        self.activations.append(output)

        return output

    def backward(self, X, y, output):
        """
        Backward propagation
        Returns gradients for weights and biases
        """
        m = X.shape[0]
        delta = output - y

        weight_gradients = []
        bias_gradients = []

        # Output layer
        weight_gradients.insert(0, np.dot(self.activations[-2].T, delta) / m)
        bias_gradients.insert(0, np.sum(delta, axis=0, keepdims=True) / m)

        # Hidden layers
        for i in range(len(self.weights)-2, -1, -1):
            delta = np.dot(delta, self.weights[i+1].T) * self.relu_derivative(self.z_values[i])
            weight_gradients.insert(0, np.dot(self.activations[i].T, delta) / m)
            bias_gradients.insert(0, np.sum(delta, axis=0, keepdims=True) / m)

        return weight_gradients, bias_gradients

    def train(self, X, y, epochs=1000):
        """Train the network"""
        for epoch in range(epochs):
            # Forward pass
            output = self.forward(X)

            # Backward pass
            weight_gradients, bias_gradients = self.backward(X, y, output)

            # Update weights and biases
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * weight_gradients[i]
                self.biases[i] -= self.learning_rate * bias_gradients[i]

            # Print progress
            if epoch % 100 == 0:
                loss = np.mean(np.square(output - y))
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

    def predict(self, X):
        """Make predictions"""
        return np.round(self.forward(X))

